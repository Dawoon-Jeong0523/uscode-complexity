# ğŸ“˜ A Century of Evolution in the Complexity of the United States Legal Code

This repository provides tools and code to reconstruct and analyze the structural evolution of the **U.S. Code** over the past century (1934â€“2023), with a particular focus on editions before the digital era (pre-1994).

## ğŸ” Overview

We leverage **OCR** and **Generative AI** techniques to recover and clean printed historical editions of the Code. This enables computational analysis of federal law even in periods before web-based digital access. The processing pipeline includes:

- ğŸ“„ **Contents of U.S. Code**: Word counts, unique word counts, entropy, scaling exponents, etc.
- ğŸŒ² **Hierarchical Structure**: Subtitle â†’ Part â†’ Chapter â†’ Section â†’ Subsection...
- ğŸ”— **Cross-Reference Relationships**: Title-to-title citation relationships

Due to repository size constraints, this GitHub includes:

- ğŸ” A sample OCR text page (`ocr_processing_gemini`) for demonstration
- ğŸŒ Web-based U.S. Code text from 1994 for structural parsing (`Data Set 2`)

The **full dataset**, including all structured data and graphs for 1934â€“2023, is hosted on Figshare:  
ğŸ‘‰ [Download full dataset (Figshare link)](XXX)

For the complete methodology and validation, please refer to our paper:  
ğŸ“„ [Read the paper (link)](XXX)

## ğŸ“ Repository Structure

```
â”œâ”€â”€ Data/                           # Sample input data and organized folders
â”‚   â”œâ”€â”€ Data Records/               # Processed datasets for each level (structure, content, citation)
â”‚   â”œâ”€â”€ OCR samples/                # Example scanned OCR text inputs
â”‚   â”œâ”€â”€ OCR_sample_processed/       # Cleaned output from OCR preprocessing
â”‚   â”œâ”€â”€ Technical Validation/       # Datasets used for technical validation
â”‚   â”œâ”€â”€ US_govinfo/                 # Raw downloaded web-based U.S. Code (1994 sample)
â”‚   â”œâ”€â”€ Title2Name.csv              # Mapping of Title numbers to their names
â”‚   â””â”€â”€ Word_count_df.csv           # Word count data for calculating scaling exponents

â”œâ”€â”€ Figures/                         # Output directory for generated figures

â”œâ”€â”€ contents_functions.py           # Functions for parsing word count and textual statistics
â”œâ”€â”€ parsing_functions.py            # Main parser: extract hierarchical structure from raw text
â”œâ”€â”€ tree_functions.py               # Tree-building, merging, and visualization utilities
â”œâ”€â”€ ocr_processing_gemini.py        # Generative-AI OCR postprocessing pipeline
â”œâ”€â”€ fallback_pdf.py                 # Backup PDF to HTML extraction tool
â”œâ”€â”€ download_html.py                # Web scraper for govinfo HTML content (post-1994)

â”œâ”€â”€ Data_Set1_Figures_part1.py      # Visualizations for Dataset 1 (content-level)
â”œâ”€â”€ Data_Set1_Figures_part2.py
â”œâ”€â”€ Data_Set1_SI_Figures.py         # Supplementary figures
â”œâ”€â”€ Data_Set2_Structure_Parsing.py  # Hierarchical structure parsing (web-based)
â”œâ”€â”€ Data_Set2_Tree_stat.py          # Structural metrics/statistics extraction
â”œâ”€â”€ Data_Set3_Edge_list.py          # Cross-reference edge list construction (citations)

â”œâ”€â”€ Technical_Validation_Figures.py # Reproduction of figures validating against prior studies

â”œâ”€â”€ requirements.txt                # Required Python libraries
â””â”€â”€ README.md                       # Project description
```

## ğŸ’» Environment

This project was developed and tested using:

- Python 3.10  
- OS: Windows 10 / Ubuntu 22.04  
- Recommended: Conda virtual environment

To recreate the environment:

```bash
conda create -n uscode-env python=3.10
conda activate uscode-env
pip install -r requirements.txt
```
